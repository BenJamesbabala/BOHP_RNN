import numpy as np
import sys
import cPickle as pickle



import rnnbohp # This is the module where the actual BOHP occurs
from rnnbohp import runNetwork # This function runs the network and computes the gradients


# Parameters. Some of them are modifiable from the command line.
NBNEUR = 29   
PATTERNSIZE = NBNEUR - 1 # Note: Apparently, pattern size must be a multiple of 4 for orthogonalization to work...
ETA = .99
RNGSEED = 0
PLASTICITY = 1
LEARNINGRATE= .003
ADAM = True
BETA1 = .9; BETA2 = .999; 
#ALPHA = .001
ALPHA = .0003
#ALPHA = .0001
WPEN1 = 0.0 #3e-4  # L1 penalty. Very bad, don't use!


# Reading command-line arguments
arguments = sys.argv[1:]
numarg = 0
while numarg < len(arguments):
    if arguments[numarg] == 'PLASTICITY':
        PLASTICITY = int(arguments[numarg+1])
    if arguments[numarg] == 'ALPHA':
        ALPHA= float(arguments[numarg+1])
    if arguments[numarg] == 'LEARNINGRATE':
        LEARNINGRATE = float(arguments[numarg+1])
    if arguments[numarg] == 'ALPHA':
        ALPHA= float(arguments[numarg+1])
    if arguments[numarg] == 'ETA':
        ETA= float(arguments[numarg+1])
    if arguments[numarg] == 'WPEN1':
        WPEN1= float(arguments[numarg+1])
    if arguments[numarg] == 'RNGSEED':
        RNGSEED = int(arguments[numarg+1])
    numarg += 2
np.random.seed(RNGSEED)


# Initializing the weights and plasticity coefficients
alpha = np.abs(np.random.rand(NBNEUR, NBNEUR)) *.01
w = np.random.randn(NBNEUR, NBNEUR) * 1.1 / np.sqrt(NBNEUR)
# Adam momentum variables
m1w = np.zeros_like(w)
m2w = np.zeros_like(w)
m1alpha = np.zeros_like(alpha)
m2alpha = np.zeros_like(alpha)


# No platic autapses ! (Fixed self-weights in w are OK)
np.fill_diagonal(alpha, 0)  


# The initial state of the network - all 0
yinit = np.random.rand(NBNEUR); yinit.fill(0)

# Other parameters
NBPRESCYCLES = 2        # Number of times each pattern is to be presented
PRESTIME = 6            # Number of time steps for each presentation
PRESTIMETEST = 6        # Same thing but for the final test pattern
INTERPRESDELAY = 4      # Duration of zero-input interval between presentations
TESTTIME = 1            # How many time steps in the final test presentations are used to compute the error
NBPATTERNS = 3          # Number of patterns to memorize in each episode
NBSTEPS = NBPRESCYCLES * ((PRESTIME + INTERPRESDELAY) * NBPATTERNS) +  PRESTIMETEST  # Total number of steps per episode
PROBADEGRADE = .5       # Proportion of bits that are zero'ed out to create the test pattern



np.set_printoptions(precision=3)
SUFFIX = "fullpattern_stronginputs_multiplepres_orthogpatterns_adam_ETA"+str(ETA)+"_TESTTIME"+str(TESTTIME)+"_NBNEUR"+str(NBNEUR)+"_PATTERNSIZE"+str(PATTERNSIZE)+"__ALPHA"+str(ALPHA)+"_NBPATTERNS"+str(NBPATTERNS)+"_LEARNINGRATE"+str(LEARNINGRATE)+"_PLASTICITY"+str(PLASTICITY)+"_RNGSEED"+str(RNGSEED)

# File to store the learning errors
myerrorfile = open("errs_"+SUFFIX+".txt", "w") 


print "Starting - "
print "Learning rate:", str(LEARNINGRATE), " RNGSEED:", str(RNGSEED), ", PLASTICITY:", str(PLASTICITY)
listerrs=[]; listtestpatterns=[]; listinputs=[]; listpatterns=[]

for numstep in range(10000):
    print numstep
    
    # Create the random patterns to be memorized in this episode
    # Orthogonal, zero-sum patterns, generated by brute force.
    # (Orthogonalizing, by itself, seems to have little impact..)
    print "Generating patterns.."
    seedp = np.ones(PATTERNSIZE); seedp[:PATTERNSIZE/2] = -1
    patterns=[]
    for nump in range(NBPATTERNS):
        sumdotsp = -1
        while sumdotsp != 0:
            p = np.random.permutation(seedp)
            sumdotsp = sum(np.abs([p.dot(pprevious) for pprevious in patterns]))
        patterns.append(p)
    print "patterns generated!"
    # Now patterns contains the NBPATTERNS patterns to be memorized in this episode
    listpatterns.append(patterns)


    # Presentation of the patterns (copy them into the appropriate positions in
    # the list of input vectors that will be fed to the network)
    inputs = [np.zeros(NBNEUR) for ii in range (NBSTEPS)]
    for ii in range(NBSTEPS):
        inputs[ii][-1] = 10.0  # Fixed-output (bias) neuron
    for nc in range(NBPRESCYCLES):
        np.random.shuffle(patterns)
        for ii in range(NBPATTERNS):
            for nn in range(PRESTIME):
                numi =nc * (NBPATTERNS * (PRESTIME+INTERPRESDELAY)) + ii * (PRESTIME+INTERPRESDELAY) + nn 
                inputs[numi][:PATTERNSIZE] = patterns[ii][:]

    # Creating the test patterns, partially zero'ed out, that the network will have to complete
    numtestpattern = np.random.randint(NBPATTERNS)
    testpattern = patterns[numtestpattern].copy()
    preservedbits = np.ones(PATTERNSIZE); preservedbits[:int(PROBADEGRADE * PATTERNSIZE)] = 0; np.random.shuffle(preservedbits)
    testpattern = testpattern *preservedbits

    for nn in range(PRESTIMETEST):
        inputs[-PRESTIMETEST + nn][:PATTERNSIZE] = testpattern[:]

    # All inputs must be very, very strong!
    for ii in inputs:
        ii  *= 20.0
    
    # Is plasticity disabled?
    if PLASTICITY == 0:
        alpha.fill(0)

    # Actually run the network and obtain the outputs and gradients of the neural outpus over the ws and alphas
    ys, xs, dydws, dydalphas = runNetwork(w, alpha, ETA, NBSTEPS, inputs=inputs, yinit=yinit)

    # Now we have the gradients of outputs over ws and alphas, we just need to backprop the error into them to obtain the appropriate parameter changes.
    # First, compute the errors for each neuron
    errs = [ys[-TESTTIME+n][:PATTERNSIZE] - patterns[numtestpattern] for n in range(TESTTIME)]  # reproduce the full test pattern, please
    # Then, compute the appropriate weight/plasticity changes (i.e. the gradient of th error over the ws and alphas)
    dws = [np.sum((errs[n] * dydws[-TESTTIME + n][:PATTERNSIZE,:,:].T).T, axis=0)  for n in range(TESTTIME)] 
    dalphas = [np.sum((errs[n] * dydalphas[-TESTTIME+ n][:PATTERNSIZE,:,:].T).T, axis=0)   for n in range(TESTTIME)]

    if ADAM:
        # The gradients are fed to an Adam solver
        dw = sum(dws) / TESTTIME
        m1w = BETA1 * m1w + (1 - BETA1) * dw
        m2w = BETA1 * m2w + (1 - BETA2) * dw * dw
        m1wcorr = m1w / (1.0 - BETA1 ** (numstep+1))
        m2wcorr = m2w / (1.0 - BETA2 ** (numstep+1))
        dalpha = sum(dalphas) / TESTTIME
        m1alpha = BETA1 * m1alpha + (1 - BETA1) * dalpha
        m2alpha = BETA1 * m2alpha + (1 - BETA2) * dalpha * dalpha
        m1alphacorr = m1alpha / (1 - BETA1 ** (numstep+1))
        m2alphacorr = m2alpha / (1 - BETA2 ** (numstep+1))
        
        # Actually modify the weights/plasticity
        w -= ALPHA * m1wcorr / (np.sqrt(m2wcorr) + 1e-8)
        alpha -= ALPHA * m1alphacorr / (np.sqrt(m2alphacorr) + 1e-8)
    else:
        # Simple SGD.
        w -= np.clip(LEARNINGRATE * sum(dws) / TESTTIME, -3e-4, 3e-4)
        alpha -= np.clip(LEARNINGRATE * sum(dalphas) / TESTTIME, -3e-4, 3e-4)
        print "Clippable ws:", np.sum(np.abs(LEARNINGRATE * sum(dws) ) / TESTTIME > 3e-4), " out of ", sum(dws).size

    # L1 penalty. Just seems to damage results, so set WPEN1 to 0.
    alpha -= WPEN1 * np.sign(alpha) 
    w -= WPEN1 * np.sign(w) 
    np.fill_diagonal(alpha, 0)  # No platic autapses!


    if (numstep+1) % 10 == 0:
        # Save the current state of learning
        params = {}
        params['w'] = w; params['alpha'] = alpha; params['errs'] = listerrs; params['testpatterns'] = listtestpatterns; params['patterns'] = listpatterns
        pickle.dump(params, open("results_"+SUFFIX+".pkl", "wb"))
    print "last PRESTIME ys[:PATTERNSIZE]:", [x[:PATTERNSIZE] for x in ys[-PRESTIME:]]
    print "test pattern:", testpattern
    #print "Inputs: ", inputs[:INPUTLENGTH]
    totalerr = np.sum(np.abs(errs))
    #listinputs.append(inputs) # Too big!
    listtestpatterns.append(testpattern)
    listerrs.append(totalerr)
    print "Err:" , totalerr
    myerrorfile.write(str(totalerr)+"\n"); myerrorfile.flush()

myerrorfile.close()


